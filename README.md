# Contrastive Learning for AttGAN and StarGAN

This project explores the application of contrastive learning to improve the performance of two popular generative adversarial networks (GANs): AttGAN and StarGAN. The primary objective is to enhance the quality and diversity of generated images by leveraging contrastive learning techniques.

## Project Overview

Generative Adversarial Networks (GANs) have revolutionized the field of image generation, enabling the creation of highly realistic images across various domains. AttGAN and StarGAN are two well-known GAN architectures that focus on different aspects of conditional image generation:

- **AttGAN**: Attribute GAN (AttGAN) is designed for facial attribute editing. It allows for precise control over specific attributes of facial images, such as age, gender, or hairstyle, while preserving other features of the image.

- **StarGAN**: StarGAN is a multi-domain image-to-image translation model capable of learning mappings among multiple domains using a single model. It is particularly useful for tasks like facial expression transformation, hairstyle changes, and other similar applications.

Contrastive learning is a self-supervised learning approach that has gained significant attention due to its ability to learn robust and meaningful representations from unlabeled data. By integrating contrastive learning into AttGAN and StarGAN, this project aims to:

1. **Enhance Image Quality**: Improve the visual fidelity and realism of generated images by encouraging the models to learn more discriminative features.
  
2. **Increase Diversity**: Promote greater diversity in the generated images by ensuring that the models generate more varied outputs across different attributes or domains.

## Methodology

1. **Contrastive Learning Integration**: The project incorporates contrastive learning into the existing architectures of AttGAN and StarGAN. The models are trained to distinguish between positive and negative pairs of images, helping them to learn more distinct and meaningful features.

2. **Data Preparation**: A dataset containing images with various attributes (for AttGAN) or domains (for StarGAN) is prepared. Positive pairs are generated by slightly altering the images within the same attribute or domain, while negative pairs are generated by selecting images from different attributes or domains.

3. **Model Training**: The models are trained using a combination of adversarial loss and contrastive loss. The contrastive loss encourages the models to reduce the distance between positive pairs and increase the distance between negative pairs in the feature space.

4. **Evaluation**: The performance of the models is evaluated using standard GAN metrics, such as Fr√©chet Inception Distance (FID) and Inception Score (IS), as well as visual inspection of the generated images.

## Results

The integration of contrastive learning is expected to result in our final paper.
